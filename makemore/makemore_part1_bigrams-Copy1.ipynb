{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b0145c-a647-4918-b1bf-70a54db9a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab2226f-8648-4b8e-ba81-7eda5130df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ee809-96c3-4667-83b4-f7ae7395b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c9357-dc83-4d0b-9d74-3489e4958e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f33096e-a66f-4e46-8153-a81345f21dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e37ad2a-4fac-431d-a415-d9c73b816fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "we iterate through the list of words, and for each word we iterate through the string by 2, but before we create special start \n",
    "and end tokens to differentiate between words and to know which character is at the start and at the end. In order to learn the\n",
    "statistics about which characters are likely to follow other characters, the simplest way in the bigram language models is to \n",
    "simply do it by counting. We are going to count how often any one of these combinations occurs in the training set.\n",
    "\n",
    "'''\n",
    "\n",
    "b = {}\n",
    "for w in words:\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        b[bigram] = b.get(bigram, 0) + 1\n",
    "        # print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc20b187-c1df-429e-8898-bfb55643365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "b.items return the tuples of ch bigrams and counts. sorted by default sorts by the first value of the tuple\n",
    "to sort by the key we use lambda! takes the key value and returns kv at 1 (not 0 but at 1, which is the count) \n",
    "and - to go backwards\n",
    "'''\n",
    "sorted(b.items(), key = lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dd039b-d1c8-420c-b1b9-b14a14a1c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a793d84b-7d48-4c0f-b4e2-79180238eda4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8351df5-0482-4e0a-8fd3-16f0d6fb115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the characters are strings, but we have to index into an array using integers, so we need a lookup table \n",
    "from characters to integers\n",
    "\n",
    "we concatenate words using join with '', this returns the entire dataset as a massive string. passing it to the set constructor\n",
    "throws out duplicates from our jointed string, because sets do not allow duplicates. so it will be the set of the lowercase\n",
    "characters. we dont actually want a set we want a list, then we sort it from a to z.\n",
    "\n",
    "now we want the mentioned lookup table\n",
    "\n",
    "we create the stoi variable (string to int). enumerate give us the iterator over the integer index and the actual element of the\n",
    "list and then we are mapping the character to the integer\n",
    "\n",
    "the special token have position 0, so we offset the other characters by +1.\n",
    "'''\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4741191-a395-46ff-801d-6deffcc6ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75316505-4294-41c8-8113-44e2746ceeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\n",
    "plt.axis('off');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf1f00-a7a3-41e6-ab05-e8123c237c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "after we create the array we have all the information necessary for us to actually sample from this bigram character level model\n",
    "and roughly what we are going to do is just start following the probabilities and counts. In the beggining we start with \n",
    "the dot (our start token) so to sample the first character of a name we are looking at the first row, we have got \n",
    "the raw counts, then we need to convert to probabilities\n",
    "\n",
    "'''\n",
    "\n",
    "N[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b48703f-f618-45c8-8688-87e9c0d553a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "we convert to float because we are going to normalize the counts\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "p = N[0].float()\n",
    "p = p / p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f3dfa-8bee-4ab9-bcce-208abffc1f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "now we can sample from these distributions! we are going to use:\n",
    "https://docs.pytorch.org/docs/stable/generated/torch.multinomial.html\n",
    "\n",
    "roughly its is: you give me probabilities and i will give you integers, which are sampled according with the distribution.\n",
    "\n",
    "to make it deterministic we will use a generator: https://docs.pytorch.org/docs/stable/generated/torch.Generator.html\n",
    "\n",
    "'''\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "itos[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68dfc3-12dc-47bf-9ee9-cb2f94f4bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "p = torch.rand(3, generator=g)\n",
    "p = p / p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8a835-7964-431e-a3a1-586441c80ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the matrix P will be similar to matrix N of counts, but every single row will have the probabilities normalized to 1, which \n",
    "indicates the probabily distribution of the next character given the character before it, which is defined by which row we are in.\n",
    "\n",
    "the first reason is efficiency, the second reason for this is to practice the n-dimensional tensors, their manipulation and\n",
    "something called broadcasting\n",
    "\n",
    "the first step is to grab the fp copy of N and divide all the rows such that they sum to 1\n",
    "\n",
    "but we need to be careful!\n",
    "\n",
    "P.sum() sums up all the counts of the entire matrix N and give us a single number of everything. that's not what we want divide.\n",
    "we want to simultaneously and in parallel divide all the rows by their respective sum.\n",
    "\n",
    "in order to do that we read the docs: https://docs.pytorch.org/docs/stable/generated/torch.sum.html\n",
    "\n",
    "torch.sum(input, dim, keepdim=False, *, dtype=None) → Tensor\n",
    "\n",
    "we provide an input array and the dim (dimension) that we want to sum, in particular we want to sum up over rows.\n",
    "\n",
    "one more argument to be careful is keepdim.\n",
    "\n",
    "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. \n",
    "Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).\n",
    "\n",
    "if its false then torch.sum not only makes the sum and collapses dimensions to be size 1, but in addition it squeezes out that\n",
    "dimension\n",
    "\n",
    "notice that P.shape is 27x27, so summing up across axis 0, we would be taking the zeroth dimension and summing across it, so when keepdim=True\n",
    "then we not only get the counts along the columns but also the shape of this is 1x27, a row vector. Again, the reason we get the row vector is that\n",
    "we passed in the 0 dimension and this 0 dimension becomes 1, and we've done a sum and we get a row. So basically we've done the sum vertically and\n",
    "arrived just a single 1x27 vector of counts.\n",
    "\n",
    "when we take out keepdim we just get 27, so it squeezes out that dimension and we just get a one dimensional vector of size 27\n",
    "\n",
    "we dont want 1x27 because it gives us the sum of counts across the columns, we want to sum the other way, along dimension 1, with the shape 27x1, so\n",
    "its a column vector. We are going  horizontally\n",
    "\n",
    "'''\n",
    "\n",
    "P.sum(1, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24363c1-0368-4585-a1b1-81a41e86c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "now pay attention. its possible to take a 27x27 array and divide by a 27x1 array?\n",
    "\n",
    "whether or not we can do this is determined by what's called broadcasting rules: https://docs.pytorch.org/docs/stable/notes/broadcasting.html\n",
    "\n",
    "you'll notice that there is this special definition for what's called broadcasting that for whether or not these two arrays\n",
    "can be combined in a binary op like division.\n",
    "Two tensors are “broadcastable” if the following rules hold:\n",
    " \n",
    "    Each tensor has at least one dimension (which is the case for us )\n",
    "\n",
    "    When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal,\n",
    "    one of them is 1, or one of them does not exist\n",
    "\n",
    "so lets see if the second rule holds. \n",
    "\n",
    "we need to align the two arrays and their shapes\n",
    "\n",
    "27 27\n",
    "27  1\n",
    "\n",
    "and we iterate from right to left. Each dimension must be either equal, one of them is 1, or one of them does not exist.\n",
    "In this case they are not equal but one of them is one, so its fine\n",
    "and the next iteration, they are both equal, so its fine too\n",
    "\n",
    "so all the dimensions are fine, therefore this operation is broadcastable. This operation is allowed but what does these \n",
    "arrays do when we divide 27 by 1. It takes this dimension 1 and stretches it out, copies it to match 27. So it takes this\n",
    "27x1 and copies it 27 times, in order to be 27x27 internally and does an element-wise division, which is what we want.\n",
    "\n",
    "So we expect it to normalize every single row!\n",
    "\n",
    "if we didn't use keepdim, we would be normalizing the columns, vertically\n",
    "\n",
    "we have to respect broadcasting, make sure it is broadcasting in the right direction, otherwise you will introduce very subtle\n",
    "bugs\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "P = N.float()\n",
    "#inplace operation in order to not create a new tensor\n",
    "P /= P.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b1f5f0-87a2-49cc-ac1f-1be0ae0545e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        #trained in bigrams\n",
    "        #broadcast version\n",
    "        p = P[ix]\n",
    "        # here we keep fetching a row of N, converting to float and dividing every single iteration, which is expensive, thats why we have the P matrix\n",
    "        #p = N[ix].float()\n",
    "        #p = p / p.sum()\n",
    "\n",
    "        #untrained\n",
    "        # p = torch.ones(27) / 27.0\n",
    "        \n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca53a72-2fdd-45dd-abbb-3b223021974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "then after learning about internals of broadcast and vector normalization, we trained a bigram language model by just \n",
    "counting how frequently any pairing occurs and then normalizing so that we get a nice property distribution. So really the \n",
    "elements of array P are really the parameters of our bigram language model, giving us and summarizing the statistics. So \n",
    "we trained and know how to sample from the model, we iteratively just sample from next character and feed it in each time \n",
    "and get a next character.\n",
    "\n",
    "now to summarize the quality of this model into a single number, how good it predicts the training set, for example. We can\n",
    "now evaluate the training loss, which tells us the quality of this model, just like we saw in micrograd.\n",
    "\n",
    "we use the log likelihood, which is the product of all probabilities inserted in a log func. the reason to use it is for\n",
    "convenience bc we have mathematically that:\n",
    "\n",
    "log(a*b*c) = log(a) + log(b) + log(c)\n",
    "\n",
    "the likelihood is the product of probabilities and the log likelihood is just the sum of the logs of the individual \n",
    "probabilities \n",
    "\n",
    "so how high can log likelihood get? when all probabilities are 1, log likelihood will be zero and then when all probabilities\n",
    "are lower, this will grow more and more negative. We don't actually like this because what we'd like is a loss function, and\n",
    "the loss function has the semantics that low is good, because we are trying to minimize the loss, so we actually need to\n",
    "invert this, which turns into negative loglikelihood.\n",
    "\n",
    "nll is a very nice loss function because the lowest it can get is zero and the higher the worst the predictions are\n",
    "\n",
    "'''\n",
    "log_likelihood = 0.0\n",
    "for w in words[:3]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    prob = P[ix, ix2]\n",
    "    logprob = torch.log(prob)\n",
    "    log_likelihood += logprob\n",
    "    print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
